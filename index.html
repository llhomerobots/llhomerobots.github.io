<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>CoRL 2024 - Lifelong Learning for Home Robots</title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <!-- Custom styles for this template -->
    <link href="./css/scrolling-nav.css" rel="stylesheet">
    <link href="./css/style.css" rel="author stylesheet">
    <style>
      .pc-column {
      width:270px;
      display:inline-block;
      vertical-align: top;
      }
      .pc_list_item {
      display:inline-block;
      width:200px;
      }
    </style>
  </head>
  <body id="page-top">
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
      <div class="container bar-container">
        <a class="title-head" href="#page-top">LLHomeRobots</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#about">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
            </li>
            <!--li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#callpapers">Call for Papers </a>
            </li-->
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#papers">Accepted Papers</a>
            </li>
             <!-- <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#faq">FAQ</a>
              </li> -->
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>
    <header class="headercontainer bg-primary text-white" style="padding: 0%; max-height: none; ">
      <div style="background-color: rgba(160,160,160,0.0)" class="text-center">
        <div style="padding-bottom: 6%; padding-top: 6%; background-image: url('./images/banner_new.png'); background-size: cover; background-position: center">
          <div class="container titlebox"; style="display: inline-block; background-color:rgba(0,0,0, 0.7); width:auto;">
            <p style="text-align: center; margin-bottom: 2" class="subtitle">Conference on Robot Learning (CoRL) 2024</p>
            <p style="text-align: center; margin-bottom: 2" class="title">Workshop on <b></b>Lifelong Learning for Home Robots</b></p>
            <p style="text-align: center; margin-bottom: 0" class="subtitle">Room: Orion (floor 2), 8:30 am - 12:30 pm | November 9 | Munich, Germany</p>
          </div>
        </div>
      </div>
    </header>
    <hr class="half-rule"/>
    <section id="about" style="padding:70px 0 0 0">
      <div class="container">
        <div class="row">
          <div class="col-md-10 mx-auto">
            <!-- div style="justify-content: center; text-align:center;"><img src="https://semrob.github.io/images/semrob_2024_logo.png" width="700px"></div-->
            <span class=titlesec>Abstract</span>
            <br>
            <span>
            Home robots should swiftly adapt by learning user preferences, understanding environmental constraints, and mastering new tasks. These lifelong learning processes should involve user demonstrations or dialogues, allowing the robot to receive guidance and resolve ambiguities through interaction with the user. Despite significant advancements in both high-level and low-level behavioral systems for robotics, several challenges persist regarding their applicability in home environments. Existing systems are typically designed for specific tasks and often struggle to adjust to changes or learn from ongoing experiences, rendering them less effective in dynamic, real-world settings like homes. To address these challenges, it is crucial to develop robots capable of lifelong learning. Lifelong learning enables robots to build complex, nuanced representations of their surroundings, facilitating continuous adaptation and informed decision-making based on their current environment, allowing robots to transcend the limitations of their initial training and evolve through interactions and experiences.  
            </span>
          </div>
        </div>
      </div>
    </section>
    <section id="event">
      <div class="container">
        <div class="row">
          <div class="col-md-10 mx-auto">
            <span class=titlesec>Event Information</span>
            <span>
            This is a primarily in-person workshop, held at the <i>2024 Conference on Robot Learning</i> (CoRL), in Munich, Germany on <b>9 November 2024</b>, starting at <b>08:30 CET</b>.
            </br>
            </br>
            The workshop location is the <b>Orion</b></a> room, in the <a href="https://www.scc-munich.com/en/" target="_blank"><b>Science Congress Center</b></a>, at Technical University of Munich (Garching) campus. You might find the <a href="https://www.corl.org/attending/conference-venue" target="_blank">venue information</a> on the CoRL 2024 website helpful.
            <!--
            </br>
            </br>
            Poster session location: outside the lower right-side doors of <b>Lecture Hall XX</b>.-->
            </span>
          </div>
        </div>
      </div>
  </section>
    <hr class="half-rule"/>
    <section id="speakers">
      <div class="container">
        <div class="row">
          <div class="col-md-10 mx-auto">
            <span class=titlesec> Invited Speakers</span><br>
            <div class="row">
              
              <a href="https://scholar.google.com/citations?user=zHFrndgAAAAJ" target="_blank">
                <div class="profpic speaker xlarge-1 columns">
                  <img  src="./images/speakers/zheyu_zhuang.png" class="figure-img img-fluid ">
                  <p class=profname> Zheyu Zhuang </p>
                  <p class=institution> Royal Institute of Technology (KTH) </p>
                </div>
              </a>
              
              <a href="https://albilo17.github.io" target="_blank">
                <div class="profpic speaker xlarge-1 columns">
                  <img  src="./images/speakers/alberta_longhini.png" class="figure-img img-fluid ">
                  <p class=profname> Alberta Longhini </p>
                  <p class=institution> Royal Institute of Technology (KTH) </p>
                </div>
              </a>

              <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/distributed-intelligence/team/prof-dr-georg-martius/" target="_blank">
                <div class="profpic speaker xlarge-1 columns">
                  <img  src="./images/speakers/martius.jpg" class="figure-img img-fluid ">
                  <p class=profname> Georg Martius </p>
                  <p class=institution> University of Tübingen </p>
                </div>
              </a>

              </div>
              <div class="row">
              
              <a href="https://talkingtorobots.com/yonatanbisk.html" target="_blank">
                <div class="profpic speaker xlarge-1 columns">
                  <img  src='./images/speakers/bisk.jpg' class="figure-img img-fluid ">
                  <p class=profname> Yonatan Bisk </p>
                  <p class=institution> Carnegie Mellon University </p>
                </div>
              </a>

              <a href="https://tedxiao.me" target="_blank">
                <div class="profpic speaker xlarge-1 columns">
                  <img  src="./images/speakers/xiao.jpg" class="figure-img img-fluid ">
                  <p class=profname> Ted Xiao </p>
                  <p class=institution> Google DeepMind </p>
                </div>
              </a>

              <a href="https://goldberg.berkeley.edu" target="_blank">
                <div class="profpic speaker xlarge-1 columns">
                  <img  src="./images/speakers/KenGoldberg.jpg" class="figure-img img-fluid ">
                  <p class=profname> Ken Goldberg </p>
                  <p class=institution> UC Berkeley </p>
                </div>
              </a>

            </div>
          </div>
        </div>
    </section>


    
    <hr class="half-rule"/>
    <section class="">
    <div class="container" id="schedule">
    <div class="row">
    <div class="col-md-10 mx-auto">
    <span class="titlesec"><span></span>Schedule</span>
    <br>
    <table class="table table-striped">
    <tbody>
    <tr>
    <th style="width: 21%"> Time </th>
    </tr>
    <tr>
    <td style="width: 21%">	8:40 <td/><td> Organizers <br> <b> Introductory Remarks  </b> </td>
    </tr>
    <tr>
    <td style="width: 21%">	9:00 <td/><td> Keynote 1: <a target="_blank" href="https://albilo17.github.io">Alberta Longhini</a> and <a target="_blank" href="https://scholar.google.com/citations?user=zHFrndgAAAAJ">Zheyu Zhuang</a> <br> <b> Skill learning and generalisation </b>
    <br>
    <a data-toggle="collapse" data-target="#abstract1" class="collapsed abstract" aria-expanded="false"> Abstract</a>
    <div id="abstract1" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
    We will present work on visual generalisation in robot manipulation, specifically focusing on learning from demonstrations. Due to limited resources, demonstrations often prioritise trajectory diversity over visual diversity, making the learned skills vulnerable to visual domain shifts, such as shadows, harsh lighting, distractors, and background changes. These distractors can include both self-distractors (robot-like entities) and object distractors. Here we introduce strategies of towards improve robustness against visual domain shifts. We will also discuss the importance of enabling robots to develop skills that generalize across object variations, with examples focusing on adapting to the diverse properties of garments. Beyond generalization, robots must also be able to handle unseen variations, learning from mistakes in the process. 
    </div>
    </td>
    </tr>
    <tr>
    <td style="width: 21%">	9:20 <td/><td> Keynote 2: Georg Martius <br> <b> Making good use of offline data -- on Causal Influence and Model-based Planning </b>
    <br>
    <a data-toggle="collapse" data-target="#abstract3" class="collapsed abstract" aria-expanded="false"> Abstract</a>
    <div id="abstract3" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
      I will present two methodologies for leveraging offline data. In the first part, I will introduce CAIAC, a data augmentation technique designed to enhance the robustness of offline learning algorithms. By employing counterfactual reasoning, CAIAC synthesizes feasible transitions from static datasets, thereby mitigating policy dependence on spurious correlations and improving generalization beyond the training distribution. The second part of the talk focuses on challenges of extracting goal-conditioned behaviors from unsupervised exploration data without further environmental interaction. We demonstrate that traditional goal-conditioned reinforcement learning struggles with this offline constraint due to value estimation artifacts. Addressing this, we propose an integrated approach combining model-based planning with graph-based value aggregation, which rectifies both local and global estimation errors and significantly boosts zero-shot goal-reaching performance.
    </div>
    </td>
    </tr>
    <tr>
    <td style="width: 21%">	9:40 <td/><td> Keynote 3: Yonatan Bisk <br> <b> Building Memories and Understanding Trajectories </b>
    <br>
    <a data-toggle="collapse" data-target="#abstract4" class="collapsed abstract" aria-expanded="false"> Abstract</a>
    <div id="abstract4" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
    As robots move into our lives they have to build non-parametric memories that grow as they move around the world, while still being queryable and useful for natural language interactions. Simultaneously our models need to understand fine-grained movement and interactions, again aligned to detailed and nuanced natural language.  This talk will discuss recent work on moving our models toward better representations of both larger and more nuanced worlds.
    </div>
    </td>
    </tr>
    <tr>
    <td style="width: 21%">	10:00 <td/><td> Spotlight Talks. IDs: 1,7,10,14,17 </td>
    </tr>
    <tr>
    <td style="width: 21%">	10:30 <td/><td> Coffee Break, Socializing, Posters </td>
    </tr>
    <tr>
    <td style="width: 21%">	11:00 <td/><td> Keynote 4: Ted Xiao <br> <b> What's Missing for Robot Foundation Models? </b>
    <br>
    <a data-toggle="collapse" data-target="#abstract5" class="collapsed abstract" aria-expanded="false"> Abstract</a>
    <div id="abstract5" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
    Intelligent robotics have seen tremendous progress in recent years. In this talk, I propose that trends in robot learning have historically almost exactly followed key trends in broader foundation modeling. After covering robotics projects which showcase the power of following such foundation modeling paradigms, I will focus on a few future-looking research directions which may suggest a unique future for how robot learning systems may develop differently from LLMs and VLMs.
    </div>
    </td>
    </tr>
    <tr>
    <td style="width: 21%">	11:20 <td/><td> Keynote 5: Ken Goldberg <br> <b> Is Data All You Need?: Large Robot Action Models and Good Old Fashioned Engineering </b>
    <br>
    <a data-toggle="collapse" data-target="#abstract6" class="collapsed abstract" aria-expanded="false"> Abstract</a>
    <div id="abstract6" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
    In this presentation, I share my concerns about current trends in robotics, including task definition, data collection, and experimental evaluation.  I propose that to reach expected performance levels, we will need "Good Old Fashioned Engineering (GOFE)" – modularity, algorithms, and metrics.   I'll present MANIP2, a modular systems architecture that can integrate learning with well-established procedural algorithmic primitives such as Inverse Kinematics, Kalman Filters, RANSAC outlier rejection, PID modules, etc. I’ll show how we are using MANIP to improve performance on robot manipulation tasks such as grasping, cable untangling, surgical suturing, motion planning, and bagging, and propose open directions for research.
    </div>
    </td>
    </tr>
    <tr>
    <td style="width: 21%">	11:40 <td/><td> Panel Discussion <br> <b>Panelists:</b> Ken Goldberg, Georg Martius, Yonatan Bisk, Ted Xiao, Alberta Longhini, Zheyu Zhuang</td>
    </tr>
    <tr>
    <td style="width: 21%"> 12:20 <td/><td> Organizers <br> <b> Closing Remarks </b>
    </td>
    </tr>
    </tbody>
    </table>
    </div>
    </div>
    </div>
    </section>
    <br>
    <hr class="half-rule"/>
    <section id="callpapers" style="padding: 40px 0 0 0;">
    <div class="container">
    <div class="row">
    <div class="col-md-10 mx-auto">
    <span class=titlesec>Call for Papers</span><br>
  
    <h5 style="font-weight: bold"> Submission Guidelines </h5>
    CoRL LLHomeRobots 2024 suggests <b>4+N (short) or 8+N (full) paper length</b> formats — i.e., 4 or 8 pages of main content with unlimited additional pages for references, appendices, etc. <br><br>
    Submissions will be handled through CMT: <a href="https://cmt3.research.microsoft.com/LLHOMEROBOTS2024" target="_blank">https://cmt3.research.microsoft.com/LLHOMEROBOTS2024</a><br><br>
    We will accept the official <a target="_blank" href="https://drive.google.com/file/d/1mPoPyHJWAfLlgAIEhWKov7Crywsz9c1M/view?usp=sharing">LaTeX</a> paper template, provided by CoRL 2024.
    <br><br>
    Our review process will be <b>double-blind</b>, following the CoRL 2024 paper submission policy.
    <br><br>
    All accepted papers will be invited for poster presentations; the highest-rated papers, according to the Technical Program Committee, will be given spotlight presentations. Accepted papers will be made available online on this workshop website as <b>non-archival</b> reports, allowing authors to also submit their works to future conferences or journals. We will highlight the Best Reviewer and reveal the Best Paper Award during the closing remarks at the workshop event.<br><br>

    Key areas we are targeting in this workshop include:</br>
  </br>
    <ul>
    <li>How can robots effectively learn from human demonstrations and feedback in a natural and intuitive manner?</br>
    <li>How can robots learn and adapt to the individual preferences and routines of different household members?</br>
    <li>What mechanisms are needed to ensure personalized and user-specific interactions and task performances?</br>
    <li>What representations can serve as robust and flexible memory systems that allow household robots to recall past experiences and apply this knowledge to new situations?</br>
    <li>What are the optimal strategies for balancing short-term (e.g., ‘working memory’) and long-term memory in robotic systems?</br>
    <li>How can robots autonomously detect and correct errors in task execution, with or without human intervention?</br>
    <li>What frameworks can enable robots to learn from their mistakes and improve over time?</br>
    <li>Few-shot learning or prompting for learning new behaviours</br>
    <li>Neural architectures of large models to support lifelong learning</br>
    <li>What metrics should we be focusing on?</br>
    <li>The role of reinforcement learning and LLMs in lifelong learning</br>

    </ul>


    <h5 style="font-weight: bold"> Important Dates </h5>
    <ul>
    <li style="display: list-item">
    <s><b>Submission deadline:</b> <font color="#000000">15 October 2024</font>, 23:59 AOE.</s>
    </li>
    <li style="display: list-item">
    <s><b>Author Notifications:</b> 25 October 2024, 23:59 AOE.</s>
    </li>
    <li style="display: list-item">
    <s><b>Camera Ready:</b> 3 November 2024, 23:59 AOE.</s>
    </li>
    <li style="display: list-item">
    <b>Workshop:</b> 9 November 2024, 08:30-12:00 CET
    </li>
    </ul>
    </div>
    </div>
    </div>
    </div>
    </div>
    </section>

  <hr class="half-rule"/>
    <section id="papers">
      <div class="container">
        <div class="row">
          <div class="col-md-10 mx-auto">
            <span class=titlesec>Accepted Papers</span><br>
            <ul class="listpapers">
              <li>
                (Paper ID #1) FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Jiaheng Hu, Rose Hendrix, Ali Farhadi, Aniruddha Kembhavi, Roberto Martin-Martin, Peter Stone, Kuo-Hao Zeng, Kiana Ehsani
                </span>
              </li>
              <li>
                (Paper ID #2) Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Haritheja Etukuru, Norihito Naka, Zijin Hu, Seungjae Lee, Julian Mehu, Aaron Edsinger, Christopher Paxton, Soumith Chintala, Lerrel Pinto, Nur Muhammad (Mahi) Shafiullah
                </span>
              </li>
              <li>
                (Paper ID #3) Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Venkatesh Pattabiraman, Yifeng Cao, Siddhant Haldar, Lerrel Pinto, Raunaq M Bhirangi
                </span>
              </li>
              <li>
                (Paper ID #4) AnySkin: Plug-and-play Skin Sensing for Robotic Touch
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Raunaq M Bhirangi, Venkatesh Pattabiraman, Enes Erciyes, Yifeng Cao, Tess Hellebrekers, Lerrel Pinto
                </span>
              </li>
              <li>
                (Paper ID #5) Local Policies Enable Zero-shot Long-horizon Manipulation
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Murtaza Dalal, Min Liu, Walter Talbott, Chen Chen, Deepak Pathak, Jian Zhang, Ruslan Salakhutdinov
                </span>
              </li>
              <li>
                (Paper ID #6) Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Quanting Xie, So Yeon Min, Tianyi Zhang, Kedi Xu, Aarav Bajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson, Yonatan Bisk
                </span>
              </li>
              <li>
                (Paper ID #7) DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Peiqi Liu, Zhanqiu Guo, Mohit Warke, Soumith Chintala, Christopher Paxton, Nur Muhammad (Mahi) Shafiullah, Lerrel Pinto
                </span>
              </li>
              <li>
                (Paper ID #8) Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Emilio Olivastri, Jonathan Francis, Alberto Pretto, Niko Sünderhauf, Krishan Rana
                </span>
              </li>
              <li>
                (Paper ID #9) Online Continual Learning for Interactive Instruction Following Agents
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Byeonghwi Kim, MinHyuk Seo, Jonghyun Choi
                </span>
              </li>
              <li>
                (Paper ID #10) Continuously Improving Mobile Manipulation with Autonomous Real-World RL
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Russell Mendonca, Emmanuel Panov, Bernadette Bucher, Jiuguang Wang, Deepak Pathak
                </span>
              </li>
              <li>
                (Paper ID #11) STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Nicholas Lenzen, Amogh Prashant Raut, Andrew Melnik
                </span>
              </li>
              <li>
                (Paper ID #12) Learning from Demonstrations with 3D Gaussian Splatting
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Michael Büttner, Jonathan Francis, Helge Rhodin, Andrew Melnik
                </span>
              </li>
              <li>
                (Paper ID #13) Parental Guidance: Efficient Lifelong Learning through Evolutionary Distillation
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Zhengyu Zhang, Quanquan Peng, Rosario Scalise, Bryon Boots
                </span>
              </li>
              <li>
                (Paper ID #14) OLiVia-Nav: An Online Lifelong Vision Language Approach for Mobile Robot Social Navigation
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Siddarth Narasimhan, Aaron H Tan, Daniel Choi, Goldie Nejat
                </span>
              </li>
              <li>
                (Paper ID #15) BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Chenguang Huang, Wolfram Burgard
                </span>
              </li>
              <li>
                (Paper ID #16) LaNMP: A Language-Conditioned Mobile Manipulation Benchmark for Autonomous Robots
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Ahmed Jaafar, Shreyas Sundara Raman, Yichen Wei, Sofia Juliani, Anneke Wernerfelt, Benedict Quartey, Ifrah Idrees, Jason Liu, Stefanie Tellex
                </span>
              </li>
              <li>
                (Paper ID #17) Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                George Gao, Tianyu Li, Nadia Figueroa
                </span>
              </li>
              <li>
                (Paper ID #19) Cognitive Planning for Object Goal Navigation using Generative AI Models
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Arjun P S, Andrew Melnik, G C Nandi
                </span>
              </li>
              <li>
                (Paper ID #20) Visual Rearrangement in Embodied AI with 3D Gaussian Splatting and Dense Feature Matching
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Arjun P S, Andrew Melnik, G C Nandi
                </span>
              </li>
              <li>
                (Paper ID #21) 2HandedAfforder: Learning Precise Actionable Bimanual Affordances from Human Videos
                <!--a class="linkpaper" href="./docs/corl_llhomerobots2024_cr_paper1.pdf" target="_blank"> [paper]</a-->
                <a class="linkpaper" href="#" target="_blank"> [paper]</a>
                <br>
                <span class="authorname">
                Marvin Heidinger, Snehal Jauhri, Vignesh Prasad, Georgia Chalvatzaki
                </span>
              </li>
              
            </ul>
          </div>
        </div>
      </div>
    </section>
    <hr class="half-rule"/>
    <section id="organizers">
      <div class="container">
      <div class="row">
        <div class="col-md-10 mx-auto">
          <span class=titlesec>Organizers</span><br>
          <div class="row">
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img  src=./images/organizers/andrew.jpg class="figure-img img-fluid ">
                <p class=profname>  
            <a href="https://www.linkedin.com/in/andrewmelnik">Andrew Melnik</a> </p>
            <p class=institution> University of Bremen </p>
            </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/jonathan.png class="figure-img img-fluid ">
                <p class=profname>
            <a href="#">Jonathan Francis</a></p>
            <p class=institution>Bosch Center for Artificial Intelligence</p>
            </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/krishan.jpg class="figure-img img-fluid ">
                <p class=profname>
            <a href="https://krishanrana.github.io"> Krishan Rana </a> </p>
            <p class=institution> QUT Centre for Robotics </p>
            </div>
            </a>
          </div>
          <div class="row">
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/uksang.png class="figure-img img-fluid ">
                <p class=profname>
            <a href="#">Uksang Yoo</a></p>
            <p class=institution>Carnegie Mellon University</p>
            </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/arthur.png class="figure-img img-fluid ">
                <p class=profname>
            <a href="#">Arthur Bucker</a></p>
            <p class=institution>Carnegie Mellon University</p>
            </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img  src='./images/organizers/dimity.jpg' class="figure-img img-fluid ">
                <p class="profname"> 
            <a href="#/"> Dimity Miller </a></p>
            <p class=institution> QUT Centre for Robotics </p>
            </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img  src='./images/organizers/chris.jpg' class="figure-img img-fluid ">
                <p class="profname"> 
            <a href="#/"> Chris Paxton </a></p>
            <p class=institution> Hello Robot Inc.</p>
            </div>
            </a>
            </div>
            </a>
          </div>
        </div>
      </div>
    </section>
    <!-- br>
    <hr class="half-rule"/>
    <section id="tpc">
      <div class="container">
        <div class="row">
          <div class="col-md-10 mx-auto">
            <span class=titlesec>Program Committee</span><br>
            
            <div class="pc-column">
              <ul>
                <li>N S</li>
              </ul>
            </div>
            
            <div class="pc-column" style="margin:0 30px 0 0;">
              <ul>
                <li>N S</li>
              </ul>
            </div>
            
            <div class="pc-column">
              <ul>
                <li>N S</li>
              </ul>
            </div>
            <br>
            <br>
            <b>ER</b> — <i>Recognises PC member who served as an Emergency Reviewer.</i><br>
            <b>TR</b> — <i>Recognises PC member who, according to Workshop Chairs' ratings, ranked in the top 10% of all Reviewers.</i><br>
            <b>MR</b> — <i>Recognises PC member who provided their services as a Meta-Reviewer.</i><br>
            <b>Ch</b> — <i>Paper Track Chair.</i>
          </div>
        </div>
      </div>
  </section -->
    <br>
    <hr class="half-rule"/>
    <section id="contact">
      <div class="container">
        <div class="row" style="width:1100px;">
          <div display="inline"><img src="https://llhomerobots.github.io/images/llhomerobots_logo.png" height="170px"></div>
          <div style="display:inline; width:520px; align-items: center; vertical-align: bottom; padding:20px 0 0 0;">
            <span class="titlesec">Contact and Information</span><br>
            <span style="justify-content: left; text-align:left;">Direct questions to <a href="mailto:llhomerobots@gmail.com">llhomerobots@gmail.com</a>.</span>
            <br><br>
            <span style="justify-content: left; text-align:left;">Subscribe to our <a target="_blank" href="https://mailchi.mp/07f8ab4c1c65/llhomerobots-workshop">mailing list</a> to stay updated on news from our workshop series.</span>
          </div>
        </div>
      </div>
  </section>
    <!-- <hr class="half-rule"/>
      <section id="sponsors">
          <div class="container">
      	<div class="row">
      	    <div class="col-md-10 mx-auto">
      	<span class=titlesec>Sponsors</span><br>
      			<img  src=../images/sponsors/nvidia.png style="width:400px;height:220px;">
          </div>
      	</div>
      	</div>
      </section> -->
    <!-- Footer -->
    <!-- Bootstrap core JavaScript -->
    <!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
      <script src="vendor/jquery/jquery.min.js"></script>
      <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
      <!-- Plugin JavaScript -->
    <!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> --> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js
      "> </script>
    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>
  </body>
</html>
